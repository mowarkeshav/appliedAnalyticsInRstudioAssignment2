---
title: "Keshav's R Journal"
author: "Keshav Mowar"
format: html
title-block-banner: true
title-block-banner-color: "white"
title-block-categories: true
---

# Date:-2nd February 2024
# Time:-11:23 PM
## Trying out some R code from the lab videos

Since R is a scripting language and I have pretty much gotten used to compiled languages it was a little difficult getting used to running separate lines of code and directing it to the console to get the output.

For now I have tried below lines of code just to get myself used to the console:-

```{r}
a <- 5
b <- 6
z <- a + b
z <- z + 11
z
```


# Date:-3rd February 2024
# Time:-10:04 PM
## Attempting to code a calculator in R

### I tried making a simple calculator using R but failed miserably. I borrowed some code from <https://www.geeksforgeeks.org/taking-input-from-user-in-r-programming/>

I got a warning message when I tried to convert my input from string to a number


```{r}
#taking the below code from this URL https://www.geeksforgeeks.org/taking-input-from-user-in-r-programming/

var <- readline()
var <- as.integer(var)
var

#building on above code to create a simple calculator

a <- readline()
a <- as.integer(var)
a
b <- readline()
b
b <- as.integer(var)
method_calc <- readline()
if(method_calc=="add"){y <- a+b}

```

##### unfortunately my attempt to build a calculator has to be postponed as i might have to learn the use of functions and if else statements before attempting that. I will try it very soon.

# Date:-8th February 2024
# Time:-9:31 AM
## Attempting to install libraries inside Quarto in order to run the Assignment code

```{r}
#installing the required packages. these are the packages I used in assignment 1 as well

#| label: load-packages
#| warning: true
#| echo: true
library(readr)
library(scales)
library(dplyr)
library(ggplot2)
library(tidyr)
library(RColorBrewer)
#commenting these out as they are causing error ---install.packages("tidyverse")
#commenting these out as they are causing error ---install.packages("ggplot2")
```

The libraries which are included here help in data visualization like creating bar plots, scatter plots and bar graphs etc.

Now we will see how to utilize these libraries for my data set and come up with some beautiful pie charts and bar graphs. A few of these libraries are used for plotting an the library called "RColorBrewer" is used for creating nice color palettes which then can be used for your graphs. I like this feature.

# Date:-20th February 2024
# Time:-6:44 PM
## Attempting to install libraries inside Quarto in order to run the Assignment code

##### Initially I was getting an error when trying to use the install function. I asked copilot for the reason of this error

#### Missing Repository Configuration:

When you run install.packages("tidyverse"), R needs to know where to find the package. By default, it tries to connect to CRAN, but you need to specify a mirror. To fix this, explicitly set the CRAN mirror by using the repos argument in the install.packages() function. For example: install.packages("tidyverse", repos = "[http://cran.us.r-project.org")](http://cran.us.r-project.org"))

##### So to sort this issue I replaced the original install function with the example given above and now the functions seem to be working.

```{r}
#installing the required packages. these are the packages I used in assignment 1 as well

#| label: load-packages
#| warning: true
#| echo: true
library(readr)
library(scales)
library(dplyr)
library(ggplot2)
library(tidyr)
library(RColorBrewer)
install.packages("tidyverse", repos = "http://cran.us.r-project.org")
install.packages("ggplot2", repos = "http://cran.us.r-project.org")
```



# Date:-21st February 2024
# Time:-5:00 PM
## Today I am running my code for the assignment 1 dataset.

This includes multiple bar plots and pie charts showcasing different aspects of the dataset.

#### Now lets try to read the a dataset from a tsv file. These kinds of files have table separated values. This particular dataset is purely categorical data and it has divided website URLS into different categories like Primary Category and Secondary Category.

```{r}
set.seed(50)

# source for the below code https://readr.tidyverse.org/reference/read_delim.html

#I noticed that the readtsv function has an option called "lazy" which helps reducing some extra configuration from the developer

cybersecdata1 <- readr::read_tsv('classification.tsv', col_names = TRUE,lazy=TRUE)

# I am trying out sampling on this dataset just to see how the values are selected. I am taking only 500 records for this dataset

# source for the below code https://www.digitalocean.com/community/tutorials/sample-in-r

cybersecdata1_sampled <- sample(1:nrow(cybersecdata1), 500)

#after the code is sampled

cybersecdata1_final <- cybersecdata1[cybersecdata1_sampled,]

cybersecdata1_final

#Next I group the dataset by the Primary Category column name as it will give an interesting insight. Primary Categories basically show which kind of industries or domains a website belongs to 

# source for the below code https://www.linkedin.com/pulse/usage-groupby-dplyr-r-tuhin-tapadar/

group_cybersecdata1 <- cybersecdata1_final %>% group_by(`Primary Category`)

#After grouping , I also count the number of times a particular category comes up by using the name_count variable. I also summarize the whole output to get a brief detail on how the tibble looks

group_cybersecdata1_sum <- summarise(group_cybersecdata1, name_count = n())

#I used below code to order the tibble in decreasing order so that we can see which category had the most websites. The reason for doing this is to understand the adaptation of web technology by different kinds of industries.

#https://bookdown.org/ndphillips/YaRrr/order-sorting-data.html

group_cybersecdata1_ordered <- group_cybersecdata1_sum[order(group_cybersecdata1_sum$name_count, decreasing = TRUE),]

#After ordering I decide to take the top 7 categories which come up. The reason for doing this is because they are the most populated categories and it makes it easier to a statistic visualization of the data.

groupcybersecdata1_ordered_sample_final <- head(group_cybersecdata1_ordered,7)

gd2 <- groupcybersecdata1_ordered_sample_final

#Once the final datagram is ready which is "gd2". I took a decision to draw a pie chart for this dataset. I very well know that pie charts are not really a good way to show data visualization but since this is a non numerical dataset and I am new to R, this is the simplest way I could find to show the dataset.

#To make a simple pie chart , we have to initialize a few variables. One of them will be the content of the pie chart(could be numbers or percentage etc) and the other will be the label of the section.

#name of the primary category
namec <- gd2$name_count
#number of websites comprising this primary category in percentage
lbls <- gd2$`Primary Category`

#setting the color pallette. 
myColorPal <- brewer.pal(5, "Set3") 

#bar graph source code below
#https://r-graph-gallery.com/131-pie-plot-with-r.html
#Finaly executing the pie chart

pie(namec,labels = paste(lbls, "\n", percent(namec / sum(namec))),col=myColorPal)
```
# Date:-25th February
#Time:-10:33 PM
## Attempting the work on my second dataset which I used in my assignment 1.

###Dataset:- https://datadrivensecurity.info/blog/pages/dds-dataset-collection.html

#### Lets talk about this dataset a little bit. The data which is shown here is taken from several honeypot servers residing in different geographical regions. Honeypot servers are usually operated by companies entice hackers and threat actors to attack their server.Keeping in mind that the threat actor might think that the server is vulnerable but in reality it is just a way to threat actors and analyze their approach to prevent a real cyber attack.
#### Hence honeypot servers are incredibly useful for cyber industry and cyber defense in general. One thing honeypot servers do is that they generate a lot of data.Only because there are so many data transmissions happening. This data definitely could give us valuable insights.
##### The data is depicted in a nice uniform format. The ip addresses are converted to long format to make it easier to read and operate. Other columns include source and destination server region names. Let see what how we can create a few visualizations using this data set.

```{r}
#I have taken this dataset to showcase some of the data transmission which happens on honeypot servers and get some insights 

#the second dataset is taken from https://datadrivensecurity.info/blog/pages/dds-dataset-collection.html



#reading csv
cybersecdata2 <- read.csv("marx.csv")

#summary and head command just to get a sense of data
summary(cybersecdata2)
head(cybersecdata2,10)

#The date format is a bit weird so I decided to make it simpler.

cybersecdata2$datetime<- as.Date(cybersecdata2$datetime, format = "%Y-%m-%d")

#dataset after changing date format

head(cybersecdata2,10)

#lets sample the data to get fewer records
cybersecdata2_sampled<- sample(1:nrow(cybersecdata2), 5000)
cybersecdata2_final <- cybersecdata2[cybersecdata2_sampled,]
head(cybersecdata2_final,10)


#grouping the hosts by region and taking their counts.for this analysis I decided to group servers by their region. this will tell us how many data transmissions happened in which region.

hosts_groupby <- cybersecdata2_final %>% group_by(host)
hosts_sum <- summarise(hosts_groupby, name_count = n())

#plotting a bar plot for showcasing server location and the number of servers in each location

ggplot(hosts_sum, aes(x = hosts_sum$host, y = hosts_sum$name_count)) +
  geom_bar(stat = "identity", fill = "purple") +  # Bar plot with identity statistic
  labs(x = "Geographical region of Host servers", y = "Number of Hosts") +  # Labels for axes
  ggtitle("Bar Plot of regional Honeypot hosts vs. Number of hosts in particular region") +  # Title for the plot
  theme_minimal()  

#below visualizations will show simple freququncy of data transmission in terms of ports and protocols. for example which protocol was high frequency etc.
#plotting a histogram to show the relation between source port number ranges and their values
hist(cybersecdata2_final$spt, 
     main = "Frequency of source ports probed in the honeypot servers",
     xlab = "Port Number Range",
     border = "purple", 
     col = "orange",)
#plotting a histogram to show the relation between destination port number ranges and their values
hist(cybersecdata2_final$dpt, 
     main = "Frequency of destination ports probed in the honeypot servers",
     xlab = "Port Number Range",
     border = "pink", 
     col = "green",)


#now I am grouping different protocols and try to show their percentage share in the data collected using both a bar plot and a  pie chart
#PLEASE NOTE THAT I AM NOT TAKING SAMPLED DATA HERE. I AM RUNNING THIS ON THE ORIGINAL DATASET.
proto_groupby <- cybersecdata2 %>% group_by(proto)
proto_sum <- summarise(proto_groupby, num_proto = n())

#this is the bar plot showing the freuqncy of different 3 protocols
ggplot(proto_sum, aes(x = proto_sum$proto, y = proto_sum$num_proto)) +
  geom_bar(stat = "identity", fill = "yellow") +  # Bar plot with identity statistic
  labs(x = "Name of protocols", y = "Frequency of occurence") +  # Labels for axes
  ggtitle("Protocols observed vs. frequency of them occuring") +  # Title for the plot
  theme_minimal()


#plotting a pie chart for the above scenario
frequency_of_proto <- proto_sum$num_proto
proto_lbls <- proto_sum$proto
myColorPal2 <- brewer.pal(5, "Set3")
pie(frequency_of_proto, labels = paste(proto_lbls, "\n", percent(frequency_of_proto / sum(frequency_of_proto))), col = myColorPal2)


```
# Date:-6th March 2024
# Time:-5:20 PM
## Learning how to add images to quarto document

I wanted to learn how to add images easily in Markdown and I found the below website really helpful. All I needed to remember was to put the image file in my working directory If I am trying to avoid writing the full path of the file.

https://quarto.org/docs/authoring/figures.html

![Adding Images for the first time](addingimages.png)



# Date:-7th March 2024

# Time:-2:30 PM

## Made a new repository in github for assignment 3

For today it is just a sample text file commit. Later throughout assignment 3 I will be pushing whatever code I will be writing.

![Github First Ever Commit](gitcode.png)



